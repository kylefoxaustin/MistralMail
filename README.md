# MistralMail - Personal Email AI Assistant

A fine-tuned Mistral 7B model trained on personal email data to generate responses in your unique writing style.

## ğŸš€ Features

- **Personalized Writing Style**: Trained on 100,000 personal emails
- **Efficient LoRA Fine-tuning**: Only 336MB adapter vs 14GB full model
- **Web Interface**: Simple Flask-based UI for testing
- **Docker Support**: Easy deployment with GPU acceleration
- **Fast Inference**: 4-bit quantization for efficient generation

## ğŸ“Š Training Details

- **Base Model**: Mistral-7B-v0.1
- **Training Method**: QLoRA (4-bit quantization + LoRA adapters)
- **Dataset**: 100,000 personal emails
- **Training Time**: ~6 hours on RTX 5090
- **Final Loss**: 0.14
- **LoRA Rank**: 32
- **Learning Rate**: 2e-4

## ğŸ› ï¸ Installation

### Prerequisites
- NVIDIA GPU with 16GB+ VRAM
- Docker with NVIDIA Container Toolkit
- CUDA 12.1+

### Quick Start

1. Clone the repository:
```bash
git clone https://github.com/kylefoxaustin/mistral-mail.git
cd mistral-mail
```

2. Download or place your trained model in `models/mistral-editorial-final/`

3. Build and run with Docker:
```bash
docker build -t mistral-mail .
docker run -it --gpus all -p 8081:8081 mistral-mail
```

4. Open http://localhost:8081 in your browser

## ğŸ‹ï¸ Training Your Own Model

See [TRAINING.md](TRAINING.md) for detailed instructions on fine-tuning Mistral on your own email dataset.

## ğŸ“ Project Structure
```
mistral-mail/
â”œâ”€â”€ app.py                 # Flask web interface
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ inference.py          # Standalone inference
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ Dockerfile           # Container configuration
â”œâ”€â”€ models/              # Model weights (not in repo)
â”‚   â””â”€â”€ mistral-editorial-final/
â””â”€â”€ data/                # Training data (not in repo)
```

## âš ï¸ Privacy Notice

This project is designed to train on personal email data. Never share your trained model publicly as it may generate text containing personal information from your training data.

## ğŸ”§ Configuration

Edit `config.yaml` to adjust:
- Generation parameters (temperature, top_p, etc.)
- Model paths
- Training hyperparameters

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file

## ğŸ‘¨â€ğŸ’» Maintainer

**Kyle Fox** - Austin, TX
- GitHub: [@kylefoxaustin](https://github.com/kylefoxaustin)
- Project: [MistralMail](https://github.com/kylefoxaustin/mistral-mail)

## ğŸ™ Acknowledgments

- Built with [Mistral-7B](https://mistral.ai/)
- Uses [PEFT](https://github.com/huggingface/peft) for LoRA
- Inspired by personal productivity needs

---

**Note**: This model is trained on personal data and should not be shared publicly.
